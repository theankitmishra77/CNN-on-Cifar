{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kK3alCdFflQX"
   },
   "source": [
    "### CNN on CIFR Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHCYMwwXflQd"
   },
   "source": [
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use DropOut layers.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlXJGxxJxySN",
    "outputId": "ef59cefd-0a63-42cc-b881-10663c09d392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5197198190959437454\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14509932544\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18187921042933008933\n",
      "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TLVcyNYKflQi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qWXUp4LmCnB2"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "l = 12\n",
    "num_filter = 24\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ouyjyN3CDkN1"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZ8wh8CbDnxO",
    "outputId": "fe36ef06-9d5a-47a8-ec5f-cbc675715da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5PoIcnsyDn1M"
   },
   "outputs": [],
   "source": [
    "def denseblock(input, num_filter = 12,dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12,dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "klRQ60cWZJ4B"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 90:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 70:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 50:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VIUlE7fnZRgF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_densenet_model.{epoch:02d}.h5'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eXGQmoswOvYM"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=filepath,monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "t_22db2zDn5K"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.5,\n",
    "    horizontal_flip=True,\n",
    "    shear_range = 5\n",
    "    )\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ezrC0lBLFXY0"
   },
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter)\n",
    "First_Transition = transition(First_Block, num_filter)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter)\n",
    "Second_Transition = transition(Second_Block, num_filter)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter)\n",
    "Third_Transition = transition(Third_Block, num_filter)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxKgoT5ZDn89",
    "outputId": "9557b2ac-7436-47f3-fa64-0109439da90a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 32, 32, 24)   648         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 32, 32, 24)   96          conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 32, 32, 24)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 32, 32, 12)   2592        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 32, 32, 36)   0           conv2d_64[0][0]                  \n",
      "                                                                 conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 32, 32, 36)   144         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 32, 32, 36)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 32, 32, 12)   3888        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 32, 32, 48)   0           concatenate_60[0][0]             \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 32, 32, 48)   192         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 32, 32, 48)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 32, 32, 12)   5184        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 32, 32, 60)   0           concatenate_61[0][0]             \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 32, 32, 60)   240         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 32, 32, 60)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 32, 32, 12)   6480        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 32, 32, 72)   0           concatenate_62[0][0]             \n",
      "                                                                 conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 32, 32, 72)   288         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 32, 32, 72)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 32, 32, 12)   7776        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 32, 32, 84)   0           concatenate_63[0][0]             \n",
      "                                                                 conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 32, 32, 84)   336         concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 32, 32, 84)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 32, 32, 12)   9072        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 32, 32, 96)   0           concatenate_64[0][0]             \n",
      "                                                                 conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 32, 32, 96)   384         concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 32, 32, 96)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 32, 32, 12)   10368       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 32, 32, 108)  0           concatenate_65[0][0]             \n",
      "                                                                 conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 32, 32, 108)  432         concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 32, 32, 108)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 32, 32, 12)   11664       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 32, 32, 120)  0           concatenate_66[0][0]             \n",
      "                                                                 conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 32, 32, 120)  480         concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 32, 32, 120)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 32, 32, 12)   12960       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 32, 32, 132)  0           concatenate_67[0][0]             \n",
      "                                                                 conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 32, 32, 132)  528         concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 32, 32, 132)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 32, 32, 12)   14256       activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 32, 32, 144)  0           concatenate_68[0][0]             \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 32, 32, 144)  576         concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 32, 32, 144)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 32, 32, 12)   15552       activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 32, 32, 156)  0           concatenate_69[0][0]             \n",
      "                                                                 conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 32, 32, 156)  624         concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 32, 32, 156)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 32, 32, 12)   16848       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, 32, 32, 168)  0           concatenate_70[0][0]             \n",
      "                                                                 conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 32, 32, 168)  672         concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 32, 32, 168)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 12)   18144       activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 32, 32, 180)  0           concatenate_71[0][0]             \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 180)  720         concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 32, 32, 180)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 32, 32, 12)   19440       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 32, 32, 192)  0           concatenate_72[0][0]             \n",
      "                                                                 conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 32, 32, 192)  768         concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 32, 32, 192)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 32, 32, 12)   20736       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 32, 32, 204)  0           concatenate_73[0][0]             \n",
      "                                                                 conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 32, 32, 204)  816         concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 32, 32, 204)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 32, 32, 12)   2448        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 16, 16, 12)   0           conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 16, 16, 12)   48          average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 16, 16, 12)   0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 16, 16, 12)   1296        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 16, 16, 24)   0           average_pooling2d_4[0][0]        \n",
      "                                                                 conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 16, 16, 24)   96          concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 16, 16, 24)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 16, 16, 12)   2592        activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 16, 16, 36)   0           concatenate_75[0][0]             \n",
      "                                                                 conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 16, 16, 36)   144         concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 16, 16, 36)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 16, 16, 12)   3888        activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 16, 16, 48)   0           concatenate_76[0][0]             \n",
      "                                                                 conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 16, 16, 48)   192         concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 16, 16, 48)   0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 16, 16, 12)   5184        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, 16, 16, 60)   0           concatenate_77[0][0]             \n",
      "                                                                 conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 16, 16, 60)   240         concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 16, 16, 60)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 16, 16, 12)   6480        activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 16, 16, 72)   0           concatenate_78[0][0]             \n",
      "                                                                 conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 72)   288         concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 16, 16, 72)   0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 16, 16, 12)   7776        activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 16, 16, 84)   0           concatenate_79[0][0]             \n",
      "                                                                 conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 16, 16, 84)   336         concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 16, 16, 84)   0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 16, 16, 12)   9072        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 16, 16, 96)   0           concatenate_80[0][0]             \n",
      "                                                                 conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 16, 16, 96)   384         concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 16, 16, 96)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 16, 16, 12)   10368       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 16, 16, 108)  0           concatenate_81[0][0]             \n",
      "                                                                 conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 16, 16, 108)  432         concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 16, 16, 108)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 16, 16, 12)   11664       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 16, 16, 120)  0           concatenate_82[0][0]             \n",
      "                                                                 conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 16, 16, 120)  480         concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 16, 16, 120)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 16, 16, 12)   12960       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 16, 16, 132)  0           concatenate_83[0][0]             \n",
      "                                                                 conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 16, 16, 132)  528         concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 16, 16, 132)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 16, 16, 12)   14256       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 16, 16, 144)  0           concatenate_84[0][0]             \n",
      "                                                                 conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 16, 16, 144)  576         concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 16, 16, 144)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 16, 16, 12)   15552       activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 16, 16, 156)  0           concatenate_85[0][0]             \n",
      "                                                                 conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 16, 16, 156)  624         concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 16, 16, 156)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 16, 16, 12)   16848       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 16, 16, 168)  0           concatenate_86[0][0]             \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 16, 16, 168)  672         concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 16, 16, 168)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 16, 16, 12)   18144       activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 16, 16, 180)  0           concatenate_87[0][0]             \n",
      "                                                                 conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 16, 16, 180)  720         concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 16, 16, 180)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 16, 16, 12)   19440       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_89 (Concatenate)    (None, 16, 16, 192)  0           concatenate_88[0][0]             \n",
      "                                                                 conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 16, 16, 192)  768         concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 16, 16, 192)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 16, 16, 12)   2304        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 8, 8, 12)     0           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 8, 8, 12)     48          average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 8, 8, 12)     0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 8, 8, 12)     1296        activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_90 (Concatenate)    (None, 8, 8, 24)     0           average_pooling2d_5[0][0]        \n",
      "                                                                 conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 8, 8, 24)     96          concatenate_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 8, 8, 24)     0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 8, 8, 12)     2592        activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_91 (Concatenate)    (None, 8, 8, 36)     0           concatenate_90[0][0]             \n",
      "                                                                 conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 8, 8, 36)     144         concatenate_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 8, 8, 36)     0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 8, 8, 12)     3888        activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_92 (Concatenate)    (None, 8, 8, 48)     0           concatenate_91[0][0]             \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 8, 8, 48)     192         concatenate_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 8, 8, 48)     0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 8, 8, 12)     5184        activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_93 (Concatenate)    (None, 8, 8, 60)     0           concatenate_92[0][0]             \n",
      "                                                                 conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 8, 8, 60)     240         concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 8, 8, 60)     0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 8, 8, 12)     6480        activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_94 (Concatenate)    (None, 8, 8, 72)     0           concatenate_93[0][0]             \n",
      "                                                                 conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 8, 8, 72)     288         concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 8, 8, 72)     0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 8, 8, 12)     7776        activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_95 (Concatenate)    (None, 8, 8, 84)     0           concatenate_94[0][0]             \n",
      "                                                                 conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 8, 8, 84)     336         concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 8, 8, 84)     0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 8, 8, 12)     9072        activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 8, 8, 96)     0           concatenate_95[0][0]             \n",
      "                                                                 conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 8, 8, 96)     384         concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 8, 8, 96)     0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 8, 8, 12)     10368       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_97 (Concatenate)    (None, 8, 8, 108)    0           concatenate_96[0][0]             \n",
      "                                                                 conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 8, 8, 108)    432         concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 8, 8, 108)    0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 8, 8, 12)     11664       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_98 (Concatenate)    (None, 8, 8, 120)    0           concatenate_97[0][0]             \n",
      "                                                                 conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 8, 8, 120)    480         concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 8, 8, 120)    0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 8, 8, 12)     12960       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_99 (Concatenate)    (None, 8, 8, 132)    0           concatenate_98[0][0]             \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 8, 8, 132)    528         concatenate_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 8, 8, 132)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 8, 8, 12)     14256       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_100 (Concatenate)   (None, 8, 8, 144)    0           concatenate_99[0][0]             \n",
      "                                                                 conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 8, 8, 144)    576         concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 8, 8, 144)    0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 8, 8, 12)     15552       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_101 (Concatenate)   (None, 8, 8, 156)    0           concatenate_100[0][0]            \n",
      "                                                                 conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 8, 8, 156)    624         concatenate_101[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 8, 8, 156)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 8, 8, 12)     16848       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_102 (Concatenate)   (None, 8, 8, 168)    0           concatenate_101[0][0]            \n",
      "                                                                 conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 8, 8, 168)    672         concatenate_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 8, 8, 168)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 8, 8, 12)     18144       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_103 (Concatenate)   (None, 8, 8, 180)    0           concatenate_102[0][0]            \n",
      "                                                                 conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 8, 8, 180)    720         concatenate_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 8, 8, 180)    0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 8, 8, 12)     19440       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_104 (Concatenate)   (None, 8, 8, 192)    0           concatenate_103[0][0]            \n",
      "                                                                 conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 8, 8, 192)    768         concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 8, 8, 192)    0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 8, 8, 12)     2304        activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 4, 4, 12)     0           conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 4, 4, 12)     48          average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 4, 4, 12)     0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 4, 4, 12)     1296        activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_105 (Concatenate)   (None, 4, 4, 24)     0           average_pooling2d_6[0][0]        \n",
      "                                                                 conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 4, 4, 24)     96          concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 4, 4, 24)     0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 4, 4, 12)     2592        activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_106 (Concatenate)   (None, 4, 4, 36)     0           concatenate_105[0][0]            \n",
      "                                                                 conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 4, 4, 36)     144         concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 4, 4, 36)     0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 4, 4, 12)     3888        activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_107 (Concatenate)   (None, 4, 4, 48)     0           concatenate_106[0][0]            \n",
      "                                                                 conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 4, 4, 48)     192         concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 4, 4, 48)     0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 4, 4, 12)     5184        activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_108 (Concatenate)   (None, 4, 4, 60)     0           concatenate_107[0][0]            \n",
      "                                                                 conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 4, 4, 60)     240         concatenate_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 4, 4, 60)     0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 4, 4, 12)     6480        activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_109 (Concatenate)   (None, 4, 4, 72)     0           concatenate_108[0][0]            \n",
      "                                                                 conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 4, 4, 72)     288         concatenate_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 4, 4, 72)     0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 4, 4, 12)     7776        activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_110 (Concatenate)   (None, 4, 4, 84)     0           concatenate_109[0][0]            \n",
      "                                                                 conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 4, 4, 84)     336         concatenate_110[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 4, 4, 84)     0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 4, 4, 12)     9072        activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_111 (Concatenate)   (None, 4, 4, 96)     0           concatenate_110[0][0]            \n",
      "                                                                 conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 4, 4, 96)     384         concatenate_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 4, 4, 96)     0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 4, 4, 12)     10368       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_112 (Concatenate)   (None, 4, 4, 108)    0           concatenate_111[0][0]            \n",
      "                                                                 conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 4, 4, 108)    432         concatenate_112[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 4, 4, 108)    0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 4, 4, 12)     11664       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_113 (Concatenate)   (None, 4, 4, 120)    0           concatenate_112[0][0]            \n",
      "                                                                 conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 4, 4, 120)    480         concatenate_113[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 4, 4, 120)    0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 4, 4, 12)     12960       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_114 (Concatenate)   (None, 4, 4, 132)    0           concatenate_113[0][0]            \n",
      "                                                                 conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 4, 4, 132)    528         concatenate_114[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 4, 4, 132)    0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 4, 4, 12)     14256       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_115 (Concatenate)   (None, 4, 4, 144)    0           concatenate_114[0][0]            \n",
      "                                                                 conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 4, 4, 144)    576         concatenate_115[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 4, 4, 144)    0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 4, 4, 12)     15552       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_116 (Concatenate)   (None, 4, 4, 156)    0           concatenate_115[0][0]            \n",
      "                                                                 conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 4, 4, 156)    624         concatenate_116[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 4, 4, 156)    0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 4, 4, 12)     16848       activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_117 (Concatenate)   (None, 4, 4, 168)    0           concatenate_116[0][0]            \n",
      "                                                                 conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 4, 4, 168)    672         concatenate_117[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 4, 4, 168)    0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 4, 4, 12)     18144       activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_118 (Concatenate)   (None, 4, 4, 180)    0           concatenate_117[0][0]            \n",
      "                                                                 conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 4, 4, 180)    720         concatenate_118[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 4, 4, 180)    0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 4, 4, 12)     19440       activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_119 (Concatenate)   (None, 4, 4, 192)    0           concatenate_118[0][0]            \n",
      "                                                                 conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 4, 4, 192)    768         concatenate_119[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 4, 4, 192)    0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 2, 2, 192)    0           activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 768)          0           average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           7690        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 683,794\n",
      "Trainable params: 670,354\n",
      "Non-trainable params: 13,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(1e-3),metrics=['accuracy'])\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),cooldown=0,patience=5,min_lr=0.5e-6)\n",
    "model.summary()\n",
    "\n",
    "callbacks = [ lr_reducer, lr_scheduler,checkpoint,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x41KB7l7DoA6",
    "outputId": "17723417-813a-4355-d1b9-9022d34abc43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 113s 266ms/step - loss: 1.7718 - accuracy: 0.3449 - val_loss: 2.2789 - val_accuracy: 0.2918\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 1.3689 - accuracy: 0.5023 - val_loss: 2.1380 - val_accuracy: 0.4113\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 249ms/step - loss: 1.1578 - accuracy: 0.5871 - val_loss: 1.6271 - val_accuracy: 0.5265\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 248ms/step - loss: 1.0266 - accuracy: 0.6367 - val_loss: 1.7233 - val_accuracy: 0.5083\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.9408 - accuracy: 0.6658 - val_loss: 1.1911 - val_accuracy: 0.6315\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.8732 - accuracy: 0.6919 - val_loss: 1.0492 - val_accuracy: 0.6503\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.8216 - accuracy: 0.7107 - val_loss: 0.8847 - val_accuracy: 0.7060\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.7812 - accuracy: 0.7266 - val_loss: 0.7616 - val_accuracy: 0.7383\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 249ms/step - loss: 0.7401 - accuracy: 0.7411 - val_loss: 1.3264 - val_accuracy: 0.6215\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.7149 - accuracy: 0.7505 - val_loss: 1.1555 - val_accuracy: 0.6790\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.6804 - accuracy: 0.7627 - val_loss: 0.9113 - val_accuracy: 0.7187\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.6608 - accuracy: 0.7684 - val_loss: 0.7269 - val_accuracy: 0.7595\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.6361 - accuracy: 0.7766 - val_loss: 1.0693 - val_accuracy: 0.6770\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.6165 - accuracy: 0.7840 - val_loss: 1.1634 - val_accuracy: 0.6754\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.6039 - accuracy: 0.7878 - val_loss: 0.6450 - val_accuracy: 0.7936\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 249ms/step - loss: 0.5826 - accuracy: 0.7951 - val_loss: 0.7487 - val_accuracy: 0.7700\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.5671 - accuracy: 0.8042 - val_loss: 0.8739 - val_accuracy: 0.7372\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.5542 - accuracy: 0.8059 - val_loss: 0.5930 - val_accuracy: 0.8074\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.5411 - accuracy: 0.8135 - val_loss: 0.5744 - val_accuracy: 0.8081\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.5253 - accuracy: 0.8170 - val_loss: 0.7087 - val_accuracy: 0.7852\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.5178 - accuracy: 0.8181 - val_loss: 0.7604 - val_accuracy: 0.7628\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 249ms/step - loss: 0.5049 - accuracy: 0.8239 - val_loss: 0.8754 - val_accuracy: 0.7505\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.4982 - accuracy: 0.8271 - val_loss: 0.4972 - val_accuracy: 0.8400\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 249ms/step - loss: 0.4876 - accuracy: 0.8297 - val_loss: 0.6003 - val_accuracy: 0.8153\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.4781 - accuracy: 0.8338 - val_loss: 0.7290 - val_accuracy: 0.7764\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.4676 - accuracy: 0.8367 - val_loss: 0.6055 - val_accuracy: 0.8122\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.4590 - accuracy: 0.8403 - val_loss: 0.5406 - val_accuracy: 0.8281\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.4587 - accuracy: 0.8407 - val_loss: 0.4884 - val_accuracy: 0.8409\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.4523 - accuracy: 0.8413 - val_loss: 0.9566 - val_accuracy: 0.7378\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.4347 - accuracy: 0.8497 - val_loss: 0.6672 - val_accuracy: 0.7901\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.4388 - accuracy: 0.8441 - val_loss: 0.6587 - val_accuracy: 0.8081\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.4233 - accuracy: 0.8519 - val_loss: 0.8840 - val_accuracy: 0.7606\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.4218 - accuracy: 0.8521 - val_loss: 0.5712 - val_accuracy: 0.8232\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.4112 - accuracy: 0.8574 - val_loss: 0.5233 - val_accuracy: 0.8405\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.4064 - accuracy: 0.8585 - val_loss: 0.6934 - val_accuracy: 0.7923\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.3996 - accuracy: 0.8604 - val_loss: 0.6898 - val_accuracy: 0.7998\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.3993 - accuracy: 0.8589 - val_loss: 0.7859 - val_accuracy: 0.7828\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.3889 - accuracy: 0.8652 - val_loss: 0.4807 - val_accuracy: 0.8564\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.3892 - accuracy: 0.8646 - val_loss: 0.4310 - val_accuracy: 0.8683\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.3806 - accuracy: 0.8657 - val_loss: 0.4810 - val_accuracy: 0.8502\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.3775 - accuracy: 0.8684 - val_loss: 0.7431 - val_accuracy: 0.7956\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.3740 - accuracy: 0.8687 - val_loss: 0.6117 - val_accuracy: 0.8252\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.3750 - accuracy: 0.8693 - val_loss: 0.4494 - val_accuracy: 0.8592\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.3599 - accuracy: 0.8738 - val_loss: 0.4776 - val_accuracy: 0.8515\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.3566 - accuracy: 0.8745 - val_loss: 0.5114 - val_accuracy: 0.8500\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 249ms/step - loss: 0.3569 - accuracy: 0.8747 - val_loss: 0.4333 - val_accuracy: 0.8740\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.3509 - accuracy: 0.8777 - val_loss: 0.5262 - val_accuracy: 0.8446\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.3529 - accuracy: 0.8769 - val_loss: 0.4986 - val_accuracy: 0.8514\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.3454 - accuracy: 0.8778 - val_loss: 0.4318 - val_accuracy: 0.8683\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 249ms/step - loss: 0.3424 - accuracy: 0.8796 - val_loss: 0.4528 - val_accuracy: 0.8654\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/100\n",
      "Learning rate:  0.001\n",
      "390/390 [==============================] - 97s 249ms/step - loss: 0.3370 - accuracy: 0.8813 - val_loss: 0.4915 - val_accuracy: 0.8596\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2825 - accuracy: 0.8998 - val_loss: 0.3383 - val_accuracy: 0.8974\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2635 - accuracy: 0.9065 - val_loss: 0.3438 - val_accuracy: 0.8979\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2568 - accuracy: 0.9097 - val_loss: 0.3096 - val_accuracy: 0.9056\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2557 - accuracy: 0.9085 - val_loss: 0.3188 - val_accuracy: 0.9039\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2523 - accuracy: 0.9110 - val_loss: 0.3120 - val_accuracy: 0.9061\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2462 - accuracy: 0.9129 - val_loss: 0.3287 - val_accuracy: 0.9029\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2459 - accuracy: 0.9136 - val_loss: 0.3227 - val_accuracy: 0.9039\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.2441 - accuracy: 0.9150 - val_loss: 0.3491 - val_accuracy: 0.8993\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2405 - accuracy: 0.9156 - val_loss: 0.3244 - val_accuracy: 0.9041\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 61/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2428 - accuracy: 0.9141 - val_loss: 0.3283 - val_accuracy: 0.9052\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 62/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2421 - accuracy: 0.9139 - val_loss: 0.3374 - val_accuracy: 0.9017\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 63/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2401 - accuracy: 0.9151 - val_loss: 0.3253 - val_accuracy: 0.9064\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 64/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2355 - accuracy: 0.9165 - val_loss: 0.3288 - val_accuracy: 0.9047\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 65/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.2360 - accuracy: 0.9170 - val_loss: 0.3302 - val_accuracy: 0.9052\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 66/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.2362 - accuracy: 0.9171 - val_loss: 0.3242 - val_accuracy: 0.9053\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 67/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2355 - accuracy: 0.9177 - val_loss: 0.3514 - val_accuracy: 0.9000\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 68/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2340 - accuracy: 0.9169 - val_loss: 0.3414 - val_accuracy: 0.9035\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 69/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2295 - accuracy: 0.9190 - val_loss: 0.3201 - val_accuracy: 0.9079\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 70/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2298 - accuracy: 0.9197 - val_loss: 0.3351 - val_accuracy: 0.9040\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 71/100\n",
      "Learning rate:  0.0001\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.2283 - accuracy: 0.9191 - val_loss: 0.3565 - val_accuracy: 0.9001\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 72/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.2260 - accuracy: 0.9201 - val_loss: 0.3240 - val_accuracy: 0.9079\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 73/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2197 - accuracy: 0.9219 - val_loss: 0.3232 - val_accuracy: 0.9076\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 74/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.2216 - accuracy: 0.9221 - val_loss: 0.3201 - val_accuracy: 0.9082\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 75/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2211 - accuracy: 0.9213 - val_loss: 0.3203 - val_accuracy: 0.9090\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 76/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2213 - accuracy: 0.9232 - val_loss: 0.3218 - val_accuracy: 0.9077\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 77/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2240 - accuracy: 0.9214 - val_loss: 0.3212 - val_accuracy: 0.9091\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 78/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.2217 - accuracy: 0.9220 - val_loss: 0.3204 - val_accuracy: 0.9084\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 79/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2222 - accuracy: 0.9210 - val_loss: 0.3238 - val_accuracy: 0.9068\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 80/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.2164 - accuracy: 0.9236 - val_loss: 0.3220 - val_accuracy: 0.9074\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 81/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2170 - accuracy: 0.9228 - val_loss: 0.3231 - val_accuracy: 0.9080\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 82/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2166 - accuracy: 0.9234 - val_loss: 0.3244 - val_accuracy: 0.9089\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 83/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2201 - accuracy: 0.9209 - val_loss: 0.3254 - val_accuracy: 0.9086\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 84/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2198 - accuracy: 0.9234 - val_loss: 0.3240 - val_accuracy: 0.9087\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 85/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2212 - accuracy: 0.9216 - val_loss: 0.3225 - val_accuracy: 0.9081\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 86/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2180 - accuracy: 0.9228 - val_loss: 0.3226 - val_accuracy: 0.9090\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 87/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2204 - accuracy: 0.9226 - val_loss: 0.3203 - val_accuracy: 0.9100\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 88/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2165 - accuracy: 0.9233 - val_loss: 0.3250 - val_accuracy: 0.9090\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 89/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2244 - accuracy: 0.9209 - val_loss: 0.3228 - val_accuracy: 0.9093\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 90/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 97s 250ms/step - loss: 0.2182 - accuracy: 0.9224 - val_loss: 0.3221 - val_accuracy: 0.9097\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 91/100\n",
      "Learning rate:  1e-05\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2180 - accuracy: 0.9226 - val_loss: 0.3248 - val_accuracy: 0.9093\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 92/100\n",
      "Learning rate:  1e-06\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2152 - accuracy: 0.9235 - val_loss: 0.3249 - val_accuracy: 0.9096\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 93/100\n",
      "Learning rate:  1e-06\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2194 - accuracy: 0.9215 - val_loss: 0.3257 - val_accuracy: 0.9092\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 94/100\n",
      "Learning rate:  1e-06\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2175 - accuracy: 0.9227 - val_loss: 0.3265 - val_accuracy: 0.9093\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 95/100\n",
      "Learning rate:  1e-06\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2202 - accuracy: 0.9218 - val_loss: 0.3243 - val_accuracy: 0.9093\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 96/100\n",
      "Learning rate:  1e-06\n",
      "390/390 [==============================] - 98s 250ms/step - loss: 0.2197 - accuracy: 0.9218 - val_loss: 0.3240 - val_accuracy: 0.9092\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 97/100\n",
      "Learning rate:  1e-06\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2174 - accuracy: 0.9221 - val_loss: 0.3243 - val_accuracy: 0.9089\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 98/100\n",
      "Learning rate:  1e-06\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2207 - accuracy: 0.9201 - val_loss: 0.3237 - val_accuracy: 0.9091\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 99/100\n",
      "Learning rate:  1e-06\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2218 - accuracy: 0.9213 - val_loss: 0.3225 - val_accuracy: 0.9094\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 100/100\n",
      "Learning rate:  1e-06\n",
      "390/390 [==============================] - 98s 251ms/step - loss: 0.2200 - accuracy: 0.9228 - val_loss: 0.3238 - val_accuracy: 0.9094\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1ef879b090>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1,validation_data=(X_test,y_test),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CBA76IHxySU",
    "outputId": "bc22d4a0-1323-40bf-b830-2445a79e090e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 6s 18ms/step - loss: 0.3238 - accuracy: 0.9094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32375726103782654, 0.9093999862670898]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CifarCNNDone.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
